name: Consolidated CI Pipeline

on:
  push:
    branches: ["main"]
    paths:
      - "wiki-service/**"
      - "wiki-chart/**"
      - ".github/workflows/ci-combined.yml"
  pull_request:
    branches: ["main"]
  workflow_dispatch:

jobs:
  python-quality:
    name: Python Code Quality
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.13'

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('wiki-service/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r wiki-service/requirements.txt bandit black ruff pip-audit

      - name: Run Linters and Security Scan
        run: |
          black --check wiki-service/
          ruff check wiki-service/
          bandit -r wiki-service/ -s B101
          pip-audit

  image-scan:
    name: Docker Image Scan
    runs-on: ubuntu-latest
    outputs:
      image_name: wiki-service
      image_tag: ci-scan-${{ github.run_id }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Build Docker image
        run: |
          IMAGE_TAG="ci-scan-${{ github.run_id }}"
          docker build -t "wiki-service:${IMAGE_TAG}" wiki-service/
          docker save "wiki-service:${IMAGE_TAG}" -o wiki-service.tar
          echo "Image saved to wiki-service.tar"

      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: 'wiki-service:ci-scan-${{ github.run_id }}'
          format: 'table'
          exit-code: '1'
          ignore-unfixed: true
          vuln-type: 'os,library'
          severity: 'CRITICAL,HIGH'

      - name: Upload image artifact
        uses: actions/upload-artifact@v4
        with:
          name: wiki-service-image
          path: wiki-service.tar

  helm-lint:
    name: Helm Chart Validation
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Helm
        uses: azure/setup-helm@v4

      - name: Run Helm lint
        run: helm lint ./wiki-chart --strict

  integration-test:
    name: Integration Tests (k3d)
    runs-on: ubuntu-latest
    needs: [python-quality, image-scan, helm-lint]
    if: always() && needs.python-quality.result == 'success' && needs.image-scan.result == 'success' && needs.helm-lint.result == 'success'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install and create k3d cluster
        run: |
          # Create a directory on the runner to act as the persistent storage
          mkdir -p /tmp/k3d-storage
          curl -s https://raw.githubusercontent.com/k3d-io/k3d/main/install.sh | bash
          k3d cluster create ci-cluster --image rancher/k3s:v1.29.11-k3s1 --volume /tmp/k3d-storage:/var/lib/rancher/k3s/storage@server:0

      - name: Configure k3d local-path provisioner
        run: |
          # This ConfigMap tells the k3s local-path provisioner where to create PersistentVolumes.
          # It must match the path mounted in the 'k3d cluster create' command.
          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: local-path-config
            namespace: kube-system
          data:
            config.json: |
              { "nodePathMap": [ { "node": "DEFAULT_PATH_FOR_NON_LISTED_NODES", "paths": [ "/var/lib/rancher/k3s/storage" ] } ] }
          EOF
          
          echo "Waiting for local-path-provisioner to be ready..."
          # This is the crucial step to prevent a race condition before helm install
          kubectl wait --for=condition=ready pod -l app=local-path-provisioner -n kube-system --timeout=120s

      - name: Download and import image
        uses: actions/download-artifact@v4
        with:
          name: wiki-service-image

      - name: Load image into k3d
        run: |
          k3d image load wiki-service.tar -c ci-cluster

      - name: Install Helm chart
        run: |
          # Use a dedicated namespace for production-like isolation
          kubectl create namespace ci-test
          helm install ci-release ./wiki-chart --namespace ci-test \
            --set fastapi.image.tag=${{ needs.image-scan.outputs.image_tag }} \
            --set fastapi.image.pullPolicy=Never \
            --set grafana.adminPassword=admin # Set the Grafana password as required by the assignment

      - name: Show Cluster State
        run: |
          echo "--- Initial cluster state after Helm install ---"
          kubectl get all --namespace ci-test
          echo "------------------------------------------------"

      - name: Wait for services to be ready
        id: wait-for-pods
        run: |
          echo "Waiting for all pods to be in a Ready state..."
          END_TIME=$((SECONDS + 300)) # 5 minute timeout
          while [ $SECONDS -lt $END_TIME ]; do
            echo "--- Pod Statuses ---"
            kubectl get pods -n ci-test
            READY_PODS=$(kubectl get pods -n ci-test -o 'jsonpath={..status.conditions[?(@.type=="Ready")].status}' | tr ' ' '\n' | grep -c True)
            TOTAL_PODS=$(kubectl get pods -n ci-test --no-headers | wc -l)
            if [ "$READY_PODS" -eq "$TOTAL_PODS" ] && [ "$TOTAL_PODS" -gt "0" ]; then
              echo "All pods are ready!"
              kubectl get pods -n ci-test
              exit 0
            fi
            echo "--------------------"
            sleep 10
          done
          echo "Timeout waiting for pods to become ready."
          exit 1

      - name: Debug Pods on Failure
        if: failure() && steps.wait-for-pods.outcome == 'failure'
        run: |
          echo "::error::Pod readiness check failed. Dumping debug information."
          echo "--- Describing all pods in ci-test namespace ---"
          kubectl describe pods -n ci-test
          echo "--- Describing all PVCs in ci-test namespace ---"
          kubectl describe pvc -n ci-test
          echo "--- Events in ci-test namespace ---"
          kubectl get events -n ci-test --sort-by='.lastTimestamp'
          kubectl describe pods -n ci-test
          echo "--- Logs for all pods in ci-test namespace ---"
          for pod in $(kubectl get pods -n ci-test -o jsonpath='{.items[*].metadata.name}'); do
            echo "--- Logs for $pod ---"
            kubectl logs --all-containers=true --tail=100 -n ci-test "$pod" || echo "Could not retrieve logs for $pod."
            echo "----------------------"
          done
          
      - name: Run End-to-End Tests
        run: |
            # --- Start E2E Test Script ---
            echo "--- Starting End-to-End Tests ---"
            
            # Set up port forwarding in the background
            kubectl port-forward --namespace ci-test svc/ci-release-wiki-chart-fastapi 8080:8000 >/dev/null 2>&1 &
            sleep 5 # Allow time for port-forward to establish

            BASE_URL="http://localhost:8080"
            SUCCESS_COUNT=0
            FAIL_COUNT=0

            # Helper function for tests
            run_test() {
              if eval "$1"; then
                echo "✓ PASSED: $2"
                SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
              else
                echo "✗ FAILED: $2"
                FAIL_COUNT=$((FAIL_COUNT + 1))
              fi
            }

            # 1. Test Root Endpoint
            run_test "curl -s --fail $BASE_URL/ | grep -q 'User and Post API'" "Root endpoint is accessible"

            # 2. Test User Creation
            USER_ID=$(curl -s -X POST "$BASE_URL/users" -H "Content-Type: application/json" -d '{"name": "E2E Test User"}' | jq -r '.id')
            run_test "[[ ! -z '$USER_ID' && '$USER_ID' != 'null' ]]" "POST /users creates a user"

            # 3. Test Get User
            run_test "curl -s --fail $BASE_URL/user/$USER_ID | grep -q 'E2E Test User'" "GET /user/{id} retrieves the correct user"

            # 4. Test Post Creation
            POST_ID=$(curl -s -X POST "$BASE_URL/posts" -H "Content-Type: application/json" -d '{\"user_id\": '$USER_ID', \"content\": \"E2E post content\"}' | jq -r '.post_id')
            run_test "[[ ! -z '$POST_ID' && '$POST_ID' != 'null' ]]" "POST /posts creates a post"

            # 5. Test Get Post
            run_test "curl -s --fail $BASE_URL/posts/$POST_ID | grep -q 'E2E post content'" "GET /posts/{id} retrieves the correct post"

            # 6. Test Metrics Endpoint
            METRICS=$(curl -s --fail $BASE_URL/metrics)
            run_test "echo '$METRICS' | grep -q 'users_created_total'" "GET /metrics exposes users_created_total"
            run_test "echo '$METRICS' | grep -q 'posts_created_total'" "GET /metrics exposes posts_created_total"

            # 7. Test Grafana Dashboard Endpoint (with dynamic password retrieval)
            echo "Retrieving Grafana admin password from secret..."
            GRAFANA_PASSWORD=$(kubectl get secret --namespace ci-test ci-release-wiki-chart-grafana-secret -o jsonpath="{.data.admin-password}" | base64 --decode)
            
            echo "Testing Grafana dashboard access..."
            # Check for a 200 OK, which confirms the service is working.
            run_test "curl -s -o /dev/null -w '%{http_code}' -u admin:'$GRAFANA_PASSWORD' $BASE_URL/grafana/d/creation-dashboard-678/creation | grep -q '200'" "GET /grafana/d/creation-dashboard-678/creation is accessible"

            # --- Test Summary ---
            echo "--- E2E Test Summary ---"
            echo "Passed: $SUCCESS_COUNT"
            echo "Failed: $FAIL_COUNT"
            echo "------------------------"

            if [ "$FAIL_COUNT" -ne 0 ]; then
              echo "E2E tests failed. Exiting."
              exit 1
            else
              echo "All E2E tests passed successfully!"
            fi
            # --- End E2E Test Script ---

      - name: Cleanup k3d cluster
        if: always() # Always run cleanup, even if tests fail
        run: |
          k3d cluster delete ci-cluster