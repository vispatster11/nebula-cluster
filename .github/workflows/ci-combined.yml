name: Consolidated CI Pipeline

on:
  push:
    branches: ["main"]
    paths:
      - "wiki-service/**"
      - "wiki-chart/**"
      - ".github/workflows/ci-combined.yml"
  pull_request:
    branches: ["main"]
  workflow_dispatch:

jobs:
  python-quality:
    name: Python Code Quality
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.13'

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('wiki-service/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r wiki-service/requirements.txt bandit black ruff pip-audit

      - name: Run Linters and Security Scan
        run: |
          black --check wiki-service/
          ruff check wiki-service/
          bandit -r wiki-service/ -s B101
          pip-audit

  helm-lint:
    name: Helm Chart Validation
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Helm
        uses: azure/setup-helm@v4

      - name: Run Helm lint
        run: helm lint ./wiki-chart --strict

  integration-test:
    name: Integration Tests (k3d)
    runs-on: ubuntu-latest
    needs: [python-quality, helm-lint]
    if: always() && needs.python-quality.result == 'success' && needs.helm-lint.result == 'success'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4


      # - name: Create k3d cluster
      #   uses: AbsaOSS/k3d-action@v1.2.0
      #   with:
      #     cluster-name: my-test-cluster
      #     # k3d uses the local-path-provisioner by default (it's a k3s feature)
      #     # We can also expose ingress ports if needed
      #     # ingressPort: 8080

      # - name: Verify cluster and local-path-provisioner
      #   run: |
      #     echo "Kubeconfig is available at: $KUBECONFIG"
      #     kubectl cluster-info
      #     kubectl get nodes
      #     # Check for the local-path-provisioner components
      #     kubectl get pods -n kube-system -l app=local-path-provisioner

      - name: Build and Scan Docker image
        id: build_image
        run: |
          IMAGE_TAG="ci-test-${{ github.run_id }}"
          docker build -t "wiki-service:${IMAGE_TAG}" wiki-service/
          echo "image_tag=${IMAGE_TAG}" >> $GITHUB_OUTPUT

      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: 'wiki-service:${{ steps.build_image.outputs.image_tag }}'
          format: 'table'
          exit-code: '1'
          ignore-unfixed: true
          vuln-type: 'os,library'
          severity: 'CRITICAL,HIGH'


      - name: Install and create k3d cluster
        run: |
          # Create a directory on the runner to act as the persistent storage
          mkdir -p /tmp/k3d-storage
          curl -s https://raw.githubusercontent.com/k3d-io/k3d/main/install.sh | bash
          k3d cluster create ci-cluster --image rancher/k3s:v1.29.11-k3s1 --volume /tmp/k3d-storage:/var/lib/rancher/k3s/storage@server:0

          
      - name: Configure local-path-provisioner (stable CI-safe version)
        run: |
          # Apply local-path-provisioner config
          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: local-path-config
            namespace: kube-system
          data:
            config.json: |
              { "nodePathMap": [ { "node": "DEFAULT_PATH_FOR_NON_LISTED_NODES", "paths": [ "/var/lib/rancher/k3s/storage" ] } ] }
          EOF

          echo "Waiting for local-path-provisioner pod to be created..."
          for i in {1..60}; do
            POD=$(kubectl get pod -n kube-system | grep local-path-provisioner | awk '{print $1}')
            if [ -n "$POD" ]; then
              echo "Found pod: $POD"
              break
            fi
            sleep 2
          done

          if [ -z "$POD" ]; then
            echo "ERROR: local-path-provisioner pod never appeared."
            kubectl get pods -n kube-system
            exit 1
          fi

          echo "Waiting for local-path-provisioner pod to be Ready..."
          kubectl wait --for=condition=Ready "pod/$POD" -n kube-system --timeout=180s


      - name: Install Helm chart
        run: |
          # Use a dedicated namespace for production-like isolation
          kubectl create namespace ci-test
          k3d image import "wiki-service:${{ steps.build_image.outputs.image_tag }}" -c ci-cluster
          helm install ci-release ./wiki-chart --namespace ci-test \
            --set fastapi.image.tag=${{ steps.build_image.outputs.image_tag }} \
            --set fastapi.image.pullPolicy=Never \
            --set grafana.adminPassword=admin # Set the Grafana password as required by the assignment

      - name: Show Cluster State
        run: |
          echo "--- Initial cluster state after Helm install ---"
          kubectl get all --namespace ci-test
          echo "------------------------------------------------"

      - name: Wait for services to be ready
        id: wait-for-pods
        run: |
          echo "Waiting for all deployments to be ready..."
          # Wait for the main application deployments to become available.
          # This is more reliable than checking all pods, as it ignores the test job.
          kubectl wait --for=condition=Available -n ci-test deployment --all --timeout=5m

      - name: Debug Pods on Failure
        if: failure() && steps.wait-for-pods.outcome == 'failure'
        run: |
          echo "::error::Pod readiness check failed. Dumping debug information."
          echo "--- Describing all pods in ci-test namespace ---"
          kubectl describe pods -n ci-test
          echo "--- Describing all PVCs in ci-test namespace ---"
          kubectl describe pvc -n ci-test
          echo "--- Events in ci-test namespace ---"
          kubectl get events -n ci-test --sort-by='.lastTimestamp'
          kubectl describe pods -n ci-test
          echo "--- Logs for all pods in ci-test namespace ---"
          for pod in $(kubectl get pods -n ci-test -o jsonpath='{.items[*].metadata.name}'); do
            echo "--- Logs for $pod ---"
            kubectl logs --all-containers=true --tail=100 -n ci-test "$pod" || echo "Could not retrieve logs for $pod."
            echo "----------------------"
          done
          
      - name: Run Application Tests (helm test)
        run: |
          # Use the built-in Helm test runner to execute the tests defined in test-job.yaml
          # This is the standard, idiomatic way to run tests with Helm.
          helm test ci-release --namespace ci-test --logs

      - name: Test Ingress and Grafana
        run: |
          echo "--- Testing Ingress and Grafana Dashboard ---"
          # Set up port forwarding to the k3d ingress controller
          kubectl port-forward --namespace kube-system service/k3d-ci-cluster-serverlb 8080:80 >/dev/null 2>&1 &
          sleep 5 # Allow time for port-forward to establish

          BASE_URL="http://localhost:8080"

          echo "Testing Grafana dashboard access..."
          # Check for a 200 OK, which confirms the ingress and service are working.
          # The -u admin:admin is required by the assignment.
          http_code=$(curl -s -o /dev/null -w '%{http_code}' -u admin:admin "$BASE_URL/grafana/d/creation-dashboard-678/creation")
          if [ "$http_code" -eq 200 ]; then
            echo "✓ PASSED: Grafana dashboard is accessible via ingress with status 200."
          else
            echo "✗ FAILED: Expected status 200 for Grafana dashboard via ingress, but got $http_code."
            exit 1
          fi

      - name: Cleanup k3d cluster
        if: always() # Always run cleanup, even if tests fail
        run: |
          k3d cluster delete ci-cluster