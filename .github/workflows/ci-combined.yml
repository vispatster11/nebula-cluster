name: Consolidated CI Pipeline

on:
  push:
    branches: ["main"]
    paths:
      - "wiki-service/**"
      - "wiki-chart/**"
      - ".github/workflows/ci-combined.yml"
  pull_request:
    branches: ["main"]
  workflow_dispatch:

jobs:
  python-quality:
    name: Python Code Quality
    runs-on: ubuntu-latest
    # runs-on: self-hosted
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.13'

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('wiki-service/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r wiki-service/requirements.txt bandit black ruff pip-audit

      - name: Run Linters and Security Scan
        run: |
          black --check wiki-service/
          ruff check wiki-service/
          bandit -r wiki-service/ -s B101
          pip-audit

  helm-lint:
    name: Helm Chart Validation
    runs-on: ubuntu-latest
    # runs-on: self-hosted
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Helm
        uses: azure/setup-helm@v4

      - name: Run Helm lint
        run: helm lint ./wiki-chart --strict

  integration-test:
    name: Integration Tests (k3d)
    runs-on: ubuntu-latest
    # runs-on: self-hosted
    needs: [python-quality, helm-lint]
    if: always() && needs.python-quality.result == 'success' && needs.helm-lint.result == 'success'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Build and Scan Docker image
        id: build_image
        run: |
          IMAGE_TAG="ci-test-${{ github.run_id }}"
          docker build -t "wiki-service:${IMAGE_TAG}" wiki-service/
          echo "image_tag=${IMAGE_TAG}" >> $GITHUB_OUTPUT

      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: 'wiki-service:${{ steps.build_image.outputs.image_tag }}'
          format: 'table'
          exit-code: '1'
          ignore-unfixed: true
          vuln-type: 'os,library'
          severity: 'CRITICAL,HIGH'

      - name: Install and create k3d cluster (CI-safe storage)
        run: |
          mkdir -p $HOME/bin
          HELM_VERSION=$(curl -s https://api.github.com/repos/helm/helm/releases/latest | grep '"tag_name":' | sed -E 's/.*"v([^"]+)".*/\1/')
          curl -sSL https://get.helm.sh/helm-v${HELM_VERSION}-linux-amd64.tar.gz -o /tmp/helm.tar.gz
          tar -xzvf /tmp/helm.tar.gz -C /tmp
          mv /tmp/linux-amd64/helm $HOME/bin/
          chmod +x $HOME/bin/helm

          curl -s https://raw.githubusercontent.com/k3d-io/k3d/main/install.sh | bash -s -- --no-sudo
          echo "$HOME/bin" >> $GITHUB_PATH

          mkdir -p /tmp/k3d-storage
          # Pre-set permissions on the host directory to match the container's user ID.
          # This provides an extra layer of assurance for the local-path-provisioner.
          echo "Setting ownership of /tmp/k3d-storage to 999:999"
          sudo chown -R 999:999 /tmp/k3d-storage
          curl -s https://raw.githubusercontent.com/k3d-io/k3d/main/install.sh | bash
          k3d cluster create ci-cluster \
            --image rancher/k3s:v1.29.11-k3s1 \
            --volume /tmp/k3d-storage:/tmp/k3d-storage@server:0
          

      - name: Configure local-path-provisioner for CI
        run: |
          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: local-path-config
            namespace: kube-system
          data:
            config.json: |
              {
                "setup": {
                  "command": ["chown", "-R", "999:999", "/"],
                  "image": "busybox:1.36"
                },
                 "nodePathMap": [
                  {
                    "node": "DEFAULT_PATH_FOR_NON_LISTED_NODES",
                    "paths": ["/tmp/k3d-storage"]
                  }
                ]
              }
          EOF

          # Wait for pod to appear
          for i in {1..60}; do
            POD=$(kubectl get pod -n kube-system | grep local-path-provisioner | awk '{print $1}')
            [[ -n "$POD" ]] && break
            sleep 2
          done

          kubectl wait --for=condition=Ready pod/$POD -n kube-system --timeout=180s

      - name: Install Helm chart
        run: |
          # Use a dedicated namespace for production-like isolation
          kubectl create namespace ci-test
          k3d image import "wiki-service:${{ steps.build_image.outputs.image_tag }}" -c ci-cluster
          helm install ci-release ./wiki-chart --namespace ci-test \
            --set fastapi.image.tag=${{ steps.build_image.outputs.image_tag }} \
            --set fastapi.image.pullPolicy=Never \
            --set grafana.adminPassword=admin # Set the Grafana password as required by the assignment

      - name: Show Cluster State and Events
        id: watch-pods
        run: |
          NAMESPACE=ci-test
          DURATION=240  # total duration in seconds (4 minutes)
          INTERVAL=15   # interval to check resources in seconds

          echo "--- Monitoring pod readiness in namespace $NAMESPACE ---"
          END=$((SECONDS+DURATION))

          while [ $SECONDS -lt $END ]; do
            # Get a list of all pod statuses and check for any that are not Running or Succeeded
            BAD_PODS=$(kubectl get pods -n $NAMESPACE -o jsonpath='{range .items[*]}{.metadata.name}{":"}{.status.phase}{"\n"}{end}' | grep -v -e "Running" -e "Succeeded" || true)

            if [ -z "$BAD_PODS" ]; then
              echo "âœ“ All pods are in a 'Running' or 'Succeeded' state."
              kubectl get pods -n $NAMESPACE
              exit 0
            else
              echo "--- Found non-ready pods. Watching cluster state... ---"
              kubectl get all -n $NAMESPACE
              echo "^^^"
              echo "$BAD_PODS"
              echo "--- Events ---"
              kubectl get events -n $NAMESPACE --sort-by='.metadata.creationTimestamp' | tail -n 10
            fi
            sleep $INTERVAL
          done

          echo "::error::Timeout waiting for all pods to become ready."
          exit 1

      - name: Wait for services to be ready
        id: wait-for-pods
        run: |
          echo "Waiting for all deployments to be ready..."
          # Wait for the main application deployments to become available.
          # This is more reliable than checking all pods, as it ignores the test job.
          kubectl wait --for=condition=Available -n ci-test deployment --all --timeout=2m

      - name: Debug Pods on Failure
        if: failure() && steps.wait-for-pods.outcome == 'failure'
        run: |
          echo "::error::Pod readiness check failed. Dumping debug information."
          echo "--- Describing all pods in ci-test namespace ---"
          kubectl describe pods -n ci-test
          echo "--- Describing all PVCs in ci-test namespace ---"
          kubectl describe pvc -n ci-test
          echo "--- Events in ci-test namespace ---"
          kubectl get events -n ci-test --sort-by='.lastTimestamp'
          echo "--- Describing all deployments in ci-test namespace ---"
          kubectl describe deployment -n ci-test
          echo "--- Logs for all pods in ci-test namespace ---"
          for pod in $(kubectl get pods -n ci-test -o jsonpath='{.items[*].metadata.name}'); do
            echo "--- Logs for $pod ---"
            kubectl logs --all-containers=true --tail=100 -n ci-test "$pod" || echo "Could not retrieve logs for $pod."
            echo "----------------------"
          done
          

      - name: Run Application Tests (helm test)
        run: |
          # Use the built-in Helm test runner to execute the tests defined in test-job.yaml
          # This is the standard, idiomatic way to run tests with Helm.
          # The --timeout flag prevents the pipeline from getting stuck if tests hang.
          helm test ci-release --namespace ci-test --timeout 5m

          echo "--- Fetching logs for all test pods ---"
          for pod in $(kubectl get pods -n ci-test -l "app.kubernetes.io/component=test" -o jsonpath='{.items[*].metadata.name}'); do
            echo "--- Logs for $pod ---"
            kubectl logs -n ci-test "$pod" || echo "Could not retrieve logs for $pod."
            echo "----------------------"
          done

      - name: Cleanup k3d cluster
        if: always() # Always run cleanup, even if tests fail
        run: |
          k3d cluster delete ci-cluster