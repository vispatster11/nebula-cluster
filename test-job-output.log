


bash ./tests/integration-local.sh 
--- Running Helm validation script ---
==========================================
Helm Chart Validation Tests
==========================================
--- 1. Linting Helm chart ---
==> Linting ./wiki-chart
[INFO] Chart.yaml: icon is recommended

1 chart(s) linted, 0 chart(s) failed
✓ Helm lint passed.
--- 2. Templating Helm chart for inspection ---
✓ Chart templates rendered to 'rendered-manifests.yaml'. Inspect this file to debug manifest issues.
--- Creating k3d cluster: local-cluster ---
INFO[0000] Prep: Network
INFO[0000] Created network 'k3d-local-cluster'
INFO[0000] Created image volume k3d-local-cluster-images 
INFO[0000] Starting new tools node...
INFO[0000] Starting node 'k3d-local-cluster-tools'      
INFO[0001] Creating node 'k3d-local-cluster-server-0'   
INFO[0001] Creating LoadBalancer 'k3d-local-cluster-serverlb' 
INFO[0001] Using the k3d-tools node to gather environment information 
INFO[0001] Starting new tools node...
INFO[0001] Starting node 'k3d-local-cluster-tools'      
INFO[0003] Starting cluster 'local-cluster'
INFO[0003] Starting servers...
INFO[0003] Starting node 'k3d-local-cluster-server-0'   
INFO[0007] All agents already running.
INFO[0007] Starting helpers...
INFO[0007] Starting node 'k3d-local-cluster-serverlb'   
INFO[0013] Injecting records for hostAliases (incl. host.k3d.internal) and for 3 network members into CoreDNS configmap... 
                                                                                                                           INFO[0015] Cluster 'local-cluster' created successfully! 
                                                                                                                                                                                    INFO[0015] You can now use it like this:
                                  kubectl cluster-info
                                                      --- Building and loading Docker image: wiki-service:local-1763311907 ---
[+] Building 0.9s (16/16) FINISHED                                                                                                                                                   docker:desktop-linux
 => [internal] load build definition from Dockerfile                                                                                                                                                 0.0s
 => => transferring dockerfile: 1.17kB                                                                                                                                                               0.0s 
 => [internal] load metadata for docker.io/library/python:3.13-slim                                                                                                                                  0.8s 
 => [internal] load .dockerignore                                                                                                                                                                    0.0s
 => => transferring context: 187B                                                                                                                                                                    0.0s 
 => [ 1/11] FROM docker.io/library/python:3.13-slim@sha256:85dfbf1b566b7addfe645faea9938e81a0a01a83580b0ea05fb23706357d77fb                                                                          0.0s 
 => [internal] load build context                                                                                                                                                                    0.0s 
 => => transferring context: 185B                                                                                                                                                                    0.0s 
 => CACHED [ 2/11] WORKDIR /app                                                                                                                                                                      0.0s 
 => CACHED [ 3/11] RUN apt-get update  && apt-get install -y --no-install-recommends gcc libpq-dev curl  && rm -rf /var/lib/apt/lists/*                                                              0.0s 
 => CACHED [ 4/11] COPY requirements.txt .                                                                                                                                                           0.0s 
 => CACHED [ 5/11] RUN pip install --no-cache-dir -r requirements.txt                                                                                                                                0.0s 
 => CACHED [ 6/11] COPY main.py .                                                                                                                                                                    0.0s 
 => CACHED [ 7/11] COPY database.py .                                                                                                                                                                0.0s 
 => CACHED [ 8/11] COPY models.py .                                                                                                                                                                  0.0s 
 => CACHED [ 9/11] COPY schemas.py .                                                                                                                                                                 0.0s 
 => CACHED [10/11] COPY metrics.py .                                                                                                                                                                 0.0s 
 => CACHED [11/11] RUN addgroup --system appgroup && adduser --system --ingroup appgroup --uid 1000 appuser  && chown -R appuser:appgroup /app                                                       0.0s 
 => exporting to image                                                                                                                                                                               0.0s 
 => => exporting layers                                                                                                                                                                              0.0s 
 => => writing image sha256:30f881fed938542220c186d025598ecadcf50f1e61ab3dc1ad9e19993fd6942f                                                                                                         0.0s 
 => => naming to docker.io/library/wiki-service:local-1763311907                                                                                                                                     0.0s 

View build details: docker-desktop://dashboard/build/desktop-linux/desktop-linux/m8cmufrv5xqnkdddyabcy0mgm

What's next:
    View a summary of image vulnerabilities and recommendations → docker scout quickview 
INFO[0000] Importing image(s) into cluster 'local-cluster' 
INFO[0000] Saving 1 image(s) from runtime...
INFO[0012] Importing images into nodes...
INFO[0012] Importing images from tarball '/k3d/images/k3d-local-cluster-images-20251116115206.tar' into node 'k3d-local-cluster-server-0'... 
INFO[0017] Removing the tarball(s) from image volume... 
INFO[0018] Removing k3d-tools node...
INFO[0019] Successfully imported image(s)
INFO[0019] Successfully imported 1 image(s) into 1 cluster(s) 
--- Detecting default storage class ---
✓ Found default storage class: local-path
--- Deploying application with Helm ---
namespace/local-test created
NAME: local-release
LAST DEPLOYED: Sun Nov 16 11:52:26 2025
NAMESPACE: local-test
STATUS: deployed
REVISION: 1
NOTES:
Thank you for installing the wiki-chart chart.
Your release is named local-release.

The full stack (FastAPI, PostgreSQL, Prometheus, Grafana) is being deployed.

---
Security & Quality:

This application is built and tested using a CI/CD pipeline that enforces
security and quality standards. This includes static code analysis, dependency
vulnerability scanning (pip-audit), and container image scanning (Trivy).

---
Accessing Services:

1. Get the application URL by running these commands:
   NOTE: It may take a few minutes for the LoadBalancer IP to be available.
         You can watch the status of by running 'kubectl get --namespace local-test svc -w local-release-wiki-chart-fastapi'
   export SERVICE_IP=$(kubectl get svc --namespace local-test local-release-wiki-chart-fastapi -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
   echo "FastAPI URL: http://$SERVICE_IP/"

2. Access Grafana:
   - Via Ingress (if Ingress is enabled and you have an Ingress Controller):
     The Grafana dashboard will be available at `http://<YOUR_INGRESS_IP>/grafana/`.

   - Via Port-Forwarding (for local access):
     kubectl port-forward --namespace local-test svc/local-release-wiki-chart-grafana 3000:80
     Then open http://localhost:3000 in your browser.

---
Credentials:

PostgreSQL and Grafana credentials are automatically generated if not provided during installation.

To retrieve them, run the following commands:

PostgreSQL Password:
  kubectl get secret --namespace local-test local-release-wiki-chart-postgres-secret -o jsonpath="{.data.password}" | base64 -d; echo

Grafana Password:
  kubectl get secret --namespace local-test local-release-wiki-chart-grafana-secret -o jsonpath="{.data.admin-password}" | base64 -d; echo

NOTE: For production, it is highly recommended to set your own passwords via the `values.yaml` file or with `--set` flags during installation, for example:
  --set postgresql.auth.password=YOUR_POSTGRES_PASSWORD
  --set grafana.adminPassword=YOUR_GRAFANA_PASSWORD

---
Portability Notes:

This chart uses NetworkPolicies that need to allow traffic from your cluster's
Ingress Controller. The labels for the Ingress Controller are configurable in
`values.yaml` under the `networkPolicy.ingressController` section.

The default values are set for k3d's default Traefik Ingress Controller.
If you are deploying to a different environment (e.g., GKE, AKS, or a cluster
with a different Ingress Controller), you will need to update these values
to match the labels of your Ingress Controller's pods and namespace.
✓ Helm release 'local-release' deployed.
--- Pods in namespace local-test: ---
NAME                                                   READY   STATUS    RESTARTS   AGE
local-release-wiki-chart-fastapi-56b5d7878-c98t2       1/1     Running   0          53s
local-release-wiki-chart-grafana-6965f6bc59-v6lkb      1/1     Running   0          53s
local-release-wiki-chart-postgres-0                    1/1     Running   0          53s
local-release-wiki-chart-prometheus-7b559cbf57-zzdmk   1/1     Running   0          53s
--- Running application tests (helm test) ---
NAME: local-release
LAST DEPLOYED: Sun Nov 16 11:52:26 2025
NAMESPACE: local-test
STATUS: deployed
REVISION: 1
TEST SUITE:     local-release-wiki-chart-test
Last Started:   Sun Nov 16 11:53:21 2025
Last Completed: Sun Nov 16 11:53:29 2025
Phase:          Succeeded
NOTES:
Thank you for installing the wiki-chart chart.
Your release is named local-release.

The full stack (FastAPI, PostgreSQL, Prometheus, Grafana) is being deployed.

---
Security & Quality:

This application is built and tested using a CI/CD pipeline that enforces
security and quality standards. This includes static code analysis, dependency
vulnerability scanning (pip-audit), and container image scanning (Trivy).

---
Accessing Services:

1. Get the application URL by running these commands:
   NOTE: It may take a few minutes for the LoadBalancer IP to be available.
         You can watch the status of by running 'kubectl get --namespace local-test svc -w local-release-wiki-chart-fastapi'
   export SERVICE_IP=$(kubectl get svc --namespace local-test local-release-wiki-chart-fastapi -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
   echo "FastAPI URL: http://$SERVICE_IP/"

2. Access Grafana:
   - Via Ingress (if Ingress is enabled and you have an Ingress Controller):
     The Grafana dashboard will be available at `http://<YOUR_INGRESS_IP>/grafana/`.

   - Via Port-Forwarding (for local access):
     kubectl port-forward --namespace local-test svc/local-release-wiki-chart-grafana 3000:80
     Then open http://localhost:3000 in your browser.

---
Credentials:

PostgreSQL and Grafana credentials are automatically generated if not provided during installation.

To retrieve them, run the following commands:

PostgreSQL Password:
  kubectl get secret --namespace local-test local-release-wiki-chart-postgres-secret -o jsonpath="{.data.password}" | base64 -d; echo

Grafana Password:
  kubectl get secret --namespace local-test local-release-wiki-chart-grafana-secret -o jsonpath="{.data.admin-password}" | base64 -d; echo

NOTE: For production, it is highly recommended to set your own passwords via the `values.yaml` file or with `--set` flags during installation, for example:
  --set postgresql.auth.password=YOUR_POSTGRES_PASSWORD
  --set grafana.adminPassword=YOUR_GRAFANA_PASSWORD

---
Portability Notes:

This chart uses NetworkPolicies that need to allow traffic from your cluster's
Ingress Controller. The labels for the Ingress Controller are configurable in
`values.yaml` under the `networkPolicy.ingressController` section.

The default values are set for k3d's default Traefik Ingress Controller.
If you are deploying to a different environment (e.g., GKE, AKS, or a cluster
with a different Ingress Controller), you will need to update these values
to match the labels of your Ingress Controller's pods and namespace.
--- Retrieving logs from test pod ---
=== Wiki Chart Integration Tests ===

Waiting for FastAPI service to be ready...
Attempt 1/30: Waiting...
✓ FastAPI service is ready

Test 1: GET / (root endpoint)
Response: {"message":"User and Post API","endpoints":{"POST /users":"Create a new user","POST /posts":"Create a new post","GET /user/{id}":"Get user by ID","GET /posts/{id}":"Get post by ID","GET /metrics":"Prometheus metrics"}}
✓ Root endpoint works

Test 2: POST /users (create user)
Response: {"id":1,"name":"TestUser","created_time":"2025-11-16T16:53:26.956254Z"}
✓ Create user works

Test 3: GET /user/1 (get user by ID)
Response: {"id":1,"name":"TestUser","created_time":"2025-11-16T16:53:26.956254Z"}
✓ Get user works

Test 4: POST /posts (create post)
Response: {"post_id":1,"content":"Test post","user_id":1,"created_time":"2025-11-16T16:53:27.046335Z"}
✓ Create post works

Test 5: GET /posts/1 (get post by ID)
Response: {"post_id":1,"content":"Test post","user_id":1,"created_time":"2025-11-16T16:53:27.046335Z"}
✓ Get post works

Test 6: GET /metrics (Prometheus metrics)
Response: # HELP python_gc_objects_collected_total Objects collected during gc
# TYPE python_gc_objects_collected_total counter
python_gc_objects_collected_total{generation="0"} 497.0
python_gc_objects_collected_total{generation="1"} 31.0
python_gc_objects_collected_total{generation="2"} 0.0
# HELP python_gc_objects_uncollectable_total Uncollectable objects found during GC
# TYPE python_gc_objects_uncollectable_total counter
python_gc_objects_uncollectable_total{generation="0"} 0.0
python_gc_objects_uncollectable_total{generation="1"} 0.0
python_gc_objects_uncollectable_total{generation="2"} 0.0
# HELP python_gc_collections_total Number of times this generation was collected
# TYPE python_gc_collections_total counter
python_gc_collections_total{generation="0"} 72.0
python_gc_collections_total{generation="1"} 6.0
python_gc_collections_total{generation="2"} 0.0
# HELP python_info Python platform information
# TYPE python_info gauge
python_info{implementation="CPython",major="3",minor="13",patchlevel="9",version="3.13.9"} 1.0
# HELP process_virtual_memory_bytes Virtual memory size in bytes.
# TYPE process_virtual_memory_bytes gauge
process_virtual_memory_bytes 3.35794176e+08
# HELP process_resident_memory_bytes Resident memory size in bytes.
# TYPE process_resident_memory_bytes gauge
process_resident_memory_bytes 8.1416192e+07
# HELP process_start_time_seconds Start time of the process since unix epoch in seconds.
# TYPE process_start_time_seconds gauge
process_start_time_seconds 1.76331198744e+09
# HELP process_cpu_seconds_total Total user and system CPU time spent in seconds.
# TYPE process_cpu_seconds_total counter
process_cpu_seconds_total 0.9400000000000001
# HELP process_open_fds Number of open file descriptors.
# TYPE process_open_fds gauge
process_open_fds 19.0
# HELP process_max_fds Maximum number of open file descriptors.
# TYPE process_max_fds gauge
process_max_fds 1.048576e+06
# HELP users_created_total Total number of users created
# TYPE users_created_total counter
users_created_total 1.0
# HELP users_created_created Total number of users created
# TYPE users_created_created gauge
users_created_created 1.7633119892739089e+09
# HELP posts_created_total Total number of posts created
# TYPE posts_created_total counter
posts_created_total 1.0
# HELP posts_created_created Total number of posts created
# TYPE posts_created_created gauge
posts_created_created 1.7633119892739418e+09
✓ Metrics endpoint works

Test 7: GET /api/health (Grafana health)
Response: '{
  "commit": "b5c56f6371",
  "database": "ok",
  "version": "9.0.0"
}'
✓ Grafana health endpoint works

Test 8: GET /-/healthy (Prometheus health)
Connecting to Prometheus at: http://local-release-wiki-chart-prometheus:9090
Response:   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0*   Trying 10.43.45.61:9090...
* Connected to local-release-wiki-chart-prometheus (10.43.45.61) port 9090 (#0)
> GET /-/healthy HTTP/1.1
> Host: local-release-wiki-chart-prometheus:9090
> User-Agent: curl/8.1.0-DEV
> Accept: */*
>
< HTTP/1.1 200 OK
< Date: Sun, 16 Nov 2025 16:53:27 GMT
< Content-Length: 30
< Content-Type: text/plain; charset=utf-8
<
{ [30 bytes data]
Prometheus Server is Healthy.
100    30  100    30    0     0  21141      0 --:--:-- --:--:-- --:--:-- 30000
* Connection #0 to host local-release-wiki-chart-prometheus left intact
✓ Prometheus health endpoint works

=== All tests passed ===
--- Testing Ingress Endpoints ---
Testing FastAPI root endpoint...
Response Body: {"message":"User and Post API","endpoints":{"POST /users":"Create a new user","POST /posts":"Create a new post","GET /user/{id}":"Get user by ID","GET /posts/{id}":"Get post by ID","GET /metrics":"Prometheus metrics"}}
✓ PASSED: FastAPI root is accessible via ingress with status 200.

Testing Grafana dashboard access...
Response Status: 200
✓ PASSED: Grafana dashboard is accessible via ingress with status 200.
--- Cleaning up ---
Press Enter to delete the k3d cluster...
INFO[0000] Deleting cluster 'local-cluster'
INFO[0002] Deleting cluster network 'k3d-local-cluster' 
INFO[0002] Deleting 1 attached volumes...
INFO[0002] Removing cluster details from default kubeconfig... 
INFO[0002] Removing standalone kubeconfig file (if there is one)... 
INFO[0002] Successfully deleted cluster local-cluster!
✓ Local test environment cleaned up.